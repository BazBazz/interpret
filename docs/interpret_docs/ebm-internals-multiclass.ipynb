{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBM Internals - Multiclass\n",
    "\n",
    "This is part 3 of a 3 part series describing EBM internals and how to make predictions. For part 1, click [here](./ebm-internals-regression.ipynb). For part 2, click [here](./ebm-internals-classification.ipynb).\n",
    "\n",
    "In this part 3 we'll cover multiclass, specified bin cuts, term exclusion, and unknown values. Before reading this part you should be familiar with the information in [part 1](./ebm-internals-regression.ipynb) and  [part 2](./ebm-internals-classification.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boilerplate\n",
    "from interpret import show\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "from interpret import set_visualize_provider\n",
    "from interpret.provider import InlineProvider\n",
    "set_visualize_provider(InlineProvider())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataset composed of a nominal categorical feature, and a continuous feature \n",
    "X = np.array([[\"Sudan\", \"U\", 0.75], [\"Germany\", \"U\", 1.75], [\"Sudan\", \"U\", 2.75], [None, \"U\", None]])\n",
    "y = np.array([110, 80, 70, 110]) # integer classes\n",
    "\n",
    "# Fit an EBM with no interactions\n",
    "# Specify exact bin cuts for the continuous feature\n",
    "# Exclude the middle feature during fitting\n",
    "# Eliminate the validation set to handle the small dataset\n",
    "ebm = ExplainableBoostingClassifier(\n",
    "    interactions=0, \n",
    "    feature_types=['nominal', 'nominal', [1.125, 2.75]], \n",
    "    mains=[0, 2], # this excludes the middle feature\n",
    "    validation_size=0, early_stopping_rounds=1000, min_samples_leaf=1)\n",
    "ebm.fit(X, y)\n",
    "show(ebm.explain_global())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per scikit-learn convention, we store the list of classes in the ebm.classes_ attribute as a sorted array. In this example our classes are integers, but we also accept strings as seen in part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.feature_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we passed feature_types into the \\_\\_init\\_\\_ function. Per scikit-learn convention, this is recorded unmodified in the ebm object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.feature_types_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We translated the feature_types passed to \\_\\_init\\_\\_ into actualized feature types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_names were not specified in the call to the \\_\\_init\\_\\_ function, so it was set to None following the scikit-learn convention of recording \\_\\_init\\_\\_ parameters unmodified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.feature_names_in_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we passed in a numpy array without specifying column names, the EBM created some default names. If we had passed feature_names to the __init__ function, or if we had used a Pandas dataframe, then feature_names_in_ would have contained those names.  Following scikit-learn's [SLEP007 convention](https://scikit-learn-enhancement-proposals.readthedocs.io/en/latest/slep007/proposal.html), we recorded this in ebm.feature_names_in_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.term_features_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the call to ExplainableBoostingClassifier(), mains was set to [0, 2], which means we excluded the middle feature in the list of terms that we boost on. We can see the missing feature in ebm.term_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.term_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ebm.term_names_ is also missing the middle feature since ebm.term_features_ is missing that feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.bins_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ebm.bins_ is a per-feature attribute, so the middle feature is listed here. We see however that the middle feature does not require binning since it does not affect the predictions of the model.\n",
    "\n",
    "These bins are structured the same was as in the previous parts. Two thing to note though is that the continuous feature bin cuts are the same as the ones specified in the feature_types parameter to the \\_\\_init\\_\\_ function.\n",
    "\n",
    "It is also noteworthy that the last cut we specified is exactly equal to the last feature value. In this instance where a feature value is identical to the cut value, the feature gets places into the upper bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, our intercept should be very close to the base rate. In the case of multiclass though, each class that we are predicting will have a logit value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.term_scores_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ebm.term_scores_[0] is once again the lookup table for the nominal categorical feature. For multiclass we see though that each bin contains as many logits as there are classes being predicted.\n",
    "\n",
    "The first index for the term scores of this additive term is the bin index from the nominal bin index. Missing values are once again placed in the 0th bin index, shown above as the first row.  The unknown bin is the last row of zeros.\n",
    "\n",
    "Since the first feature is a nominal categorial, we use the dictionary {'Germany': 1, 'Sudan': 2} to lookup which row of logits to use for each categorical string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ebm.term_scores_[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ebm.term_scores_[1] is for the continuous feature in our dataset.  As with categoricals, the 0th and last index (index 4) rows are for missing values, and unknown values respectively. This particular example has 5 bins (the 0th missing bin, the three partitions from the 2 cuts, and the unknown bin). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sample code encorporates everything discussed in all 3 sections. It could be used as a drop in replacement for the existing EBM predict/predict_proba functions of the EBMModel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "sample_scores = []\n",
    "# we have 4 samples in X, so loop 4 times\n",
    "for sample in X:\n",
    "    # start from the intercept for each sample\n",
    "    score = ebm.intercept_\n",
    "    if not isinstance(ebm.intercept_, float):\n",
    "        # make a copy of the ebm.intercept_ array\n",
    "        score = ebm.intercept_.copy()\n",
    "\n",
    "    # we have 2 terms, so add their score contributions\n",
    "    for term_idx, features in enumerate(ebm.term_features_):\n",
    "        # we'll be indexing into a tensor, so our index needs to be multi-dimensional\n",
    "        tensor_index = []\n",
    "        # for each feature that is a component of the term\n",
    "        for feature_idx in features:\n",
    "            feature_val = sample[feature_idx]\n",
    "\n",
    "            if feature_val is None or feature_val is np.nan:\n",
    "                # missing values are always in the 0th bin\n",
    "                bin_idx = 0\n",
    "            else:\n",
    "                # we bin differently for main effects and pairs, so first \n",
    "                # get the list containing the bins for different resolutions\n",
    "                bin_levels = ebm.bins_[feature_idx]\n",
    "\n",
    "                # what resolution do we need for this term (main resolution, \n",
    "                # pair resolution, etc.), but limit to the last resolution available\n",
    "                bins = bin_levels[min(len(bin_levels), len(features)) - 1]\n",
    "\n",
    "                if isinstance(bins, dict):\n",
    "                    # categorical feature\n",
    "                    # 'unknown' category strings are in the last bin (-1)\n",
    "                    bin_idx = bins.get(feature_val, -1)\n",
    "                else:\n",
    "                    # continuous feature\n",
    "                    try:\n",
    "                        # try converting to a float, if that fails it's 'unknown'\n",
    "                        feature_val = float(feature_val)\n",
    "                        # add 1 because the 0th bin is reserved for 'missing'\n",
    "                        bin_idx = np.digitize(feature_val, bins) + 1\n",
    "                    except ValueError:\n",
    "                        # non-floats are 'unknown', which is in the last bin (-1)\n",
    "                        bin_idx = -1\n",
    "        \n",
    "            tensor_index.append(bin_idx)\n",
    "        score_tensor = ebm.term_scores_[term_idx]\n",
    "        score += score_tensor[tuple(tensor_index)]\n",
    "    sample_scores.append(score)\n",
    "\n",
    "predictions = np.array(sample_scores)\n",
    "\n",
    "if hasattr(ebm, 'classes_'):\n",
    "    # classification\n",
    "    if len(ebm.classes_) <= 2:\n",
    "        # binary classification\n",
    "        \n",
    "        # softmax expects two logits for binary classfication\n",
    "        # the first logit is alwasy equivalent to 0 for binary classification\n",
    "        predictions = np.c_[np.zeros(predictions.shape), predictions]\n",
    "\n",
    "    predictions = softmax(predictions)\n",
    "\n",
    "if hasattr(ebm, 'classes_'):\n",
    "    print(\"probabilities for classes \" + str(ebm.classes_))\n",
    "    print(\"\")\n",
    "    print(ebm.predict_proba(X))\n",
    "else:\n",
    "    print(ebm.predict(X))\n",
    "print(\"\")\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
